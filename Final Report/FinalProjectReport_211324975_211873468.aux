\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{hendrycks2016baseline}
\citation{guo2017calibration}
\citation{muller2019does}
\citation{bohdal2021meta}
\citation{bohdal2021meta}
\citation{bohdal2021meta}
\citation{guo2017calibration}
\citation{kumar2018trainable}
\citation{mukhoti2020calibrating}
\citation{brier1950verification}
\citation{muller2019does}
\citation{bohdal2021meta}
\newlabel{sec:Introduction}{{1}{1}{Introduction}{section.1}{}}
\newlabel{sec:RelatedWork}{{2}{1}{Related work}{section.2}{}}
\newlabel{ssec:calibration}{{2}{1}{Calibration}{section*.1}{}}
\citation{gulrajani2020search}
\citation{finn2017model}
\citation{li2019feature}
\citation{wald2021calibration}
\citation{bohdal2021meta}
\newlabel{ssec:ood-gen}{{2}{2}{Out-of-domain generalization}{section*.2}{}}
\newlabel{ssec:multi-domain-calibration}{{2}{2}{Multi-domain calibration}{section*.3}{}}
\newlabel{sec:TrainingObjective}{{3}{2}{Training objective for meta calibration}{section.3}{}}
\newlabel{sec:LabelSmoothing}{{4}{2}{Meta calibration method: label smoothing}{section.4}{}}
\newlabel{ssec:LS-definition}{{4.1}{2}{label-smoothing definition}{subsection.4.1}{}}
\citation{wandb}
\citation{arjovsky2019invariant}
\citation{ghifary2015domain}
\citation{hendrycks2019benchmarking}
\citation{bohdal2021meta}
\citation{zhang2021adaptive}
\newlabel{ssec:learnable_psets}{{4.2}{3}{learnable parameter sets}{subsection.4.2}{}}
\newlabel{ssec:traing-procedure}{{4.3}{3}{training procedure}{subsection.4.3}{}}
\newlabel{sec:Experiments}{{5}{3}{Experimental setup}{section.5}{}}
\newlabel{ssec:datasets}{{5.1}{3}{Datasets}{subsection.5.1}{}}
\citation{bohdal2021meta}
\citation{gulrajani2020search}
\citation{bohdal2021meta}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:TrainProcedure}{{1}{4}{TrainProcedure\relax }{algorithm.1}{}}
\citation{bohdal2021meta}
\newlabel{fig:rotated_avg_acc}{{1}{5}{The accuracy of each model over the RotatedMNIST validation set, function of training step.\relax }{figure.caption.4}{}}
\newlabel{fig:rotated_meta_loss}{{2}{5}{The meta loss over the meta-validation set in RotatedMNIST, comparing the meta-calibration techniques. Since the regular ERM doesn't use meta-training, we set its value to zero.\relax }{figure.caption.5}{}}
\newlabel{sec:Discussion}{{7}{5}{Discussion and future work}{section.7}{}}
\citation{bohdal2021meta}
\citation{bohdal2021meta}
\newlabel{fig:rotated_ece}{{3}{6}{The ECE calibration metric over the validation set in RotatedMNIST as a function of training step.\relax }{figure.caption.6}{}}
\newlabel{fig:colored_avg_acc}{{4}{6}{The accuracy of each model over the ColoredMNIST validation set, function of training step.\relax }{figure.caption.7}{}}
\newlabel{fig:colored_ece}{{5}{7}{The ECE calibration metric over the validation set in ColoredMNIST as a function of training step.\relax }{figure.caption.8}{}}
\newlabel{tab:MD_train_UD_meta}{{1}{7}{Results over the CorruptedCIFAR10 dataset, where the train set is multi-domain and the meta-validation set is uni-domain.\relax }{table.caption.9}{}}
\bibstyle{acl_natbib}
\bibdata{references}
\bibcite{arjovsky2019invariant}{{1}{2019}{{Arjovsky et~al.}}{{Arjovsky, Bottou, Gulrajani, and Lopez-Paz}}}
\bibcite{wandb}{{2}{2020}{{Biewald}}{{}}}
\bibcite{bohdal2021meta}{{3}{2021}{{Bohdal et~al.}}{{Bohdal, Yang, and Hospedales}}}
\bibcite{brier1950verification}{{4}{1950}{{Brier et~al.}}{{}}}
\bibcite{finn2017model}{{5}{2017}{{Finn et~al.}}{{Finn, Abbeel, and Levine}}}
\bibcite{ghifary2015domain}{{6}{2015}{{Ghifary et~al.}}{{Ghifary, Kleijn, Zhang, and Balduzzi}}}
\bibcite{gulrajani2020search}{{7}{2020}{{Gulrajani and Lopez-Paz}}{{}}}
\bibcite{guo2017calibration}{{8}{2017}{{Guo et~al.}}{{Guo, Pleiss, Sun, and Weinberger}}}
\bibcite{hendrycks2019benchmarking}{{9}{2019}{{Hendrycks and Dietterich}}{{}}}
\bibcite{hendrycks2016baseline}{{10}{2016}{{Hendrycks and Gimpel}}{{}}}
\bibcite{kumar2018trainable}{{11}{2018}{{Kumar et~al.}}{{Kumar, Sarawagi, and Jain}}}
\bibcite{li2019feature}{{12}{2019}{{Li et~al.}}{{Li, Yang, Zhou, and Hospedales}}}
\bibcite{mukhoti2020calibrating}{{13}{2020}{{Mukhoti et~al.}}{{Mukhoti, Kulharia, Sanyal, Golodetz, Torr, and Dokania}}}
\newlabel{tab:MD_train_MD_meta}{{2}{8}{Results over the CorruptedCIFAR10 dataset, where the train set is multi-domain and the meta-validation set is multi-domain. Note that the no\_meta method is equivalent to tabel \ref {tab:MD_train_UD_meta}\relax }{table.caption.10}{}}
\newlabel{tab:UD_train}{{3}{8}{Results over the CorruptedCIFAR10 dataset, where the train set is uni-domain. Therefore the md\_meta is equivalent to the one\_vec\_meta.\relax }{table.caption.11}{}}
\newlabel{sec:Conclusion}{{8}{8}{Conclusion}{section.8}{}}
\bibcite{muller2019does}{{14}{2019}{{M{\"u}ller et~al.}}{{M{\"u}ller, Kornblith, and Hinton}}}
\bibcite{wald2021calibration}{{15}{2021}{{Wald et~al.}}{{Wald, Feder, Greenfeld, and Shalit}}}
\bibcite{zhang2021adaptive}{{16}{2021}{{Zhang et~al.}}{{Zhang, Marklund, Dhawan, Gupta, Levine, and Finn}}}
\gdef \@abspage@last{9}
